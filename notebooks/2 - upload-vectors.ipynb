{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import opensearchpy\n",
    "\n",
    "chunk_max_words = 500\n",
    "model_name = \"distiluse-base-multilingual-cased-v2\"\n",
    "index_name =  os.environ.get('AWS_OPENSEARCH_INDEX_NAME')\n",
    "service = 'aoss'\n",
    "host = os.getenv('AWS_OPENSEARCH_ENDPOINT')\n",
    "region = os.getenv('AWS_REGION')\n",
    "\n",
    "chunks_dir = \"../data/split_chunks\"\n",
    "\n",
    "access_key = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as IAM identity: arn:aws:iam::211125557955:user/Christophe\n",
      "Index fed-court-chunks-index exists: True\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenSearch client\n",
    "\n",
    "\n",
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "awsauth = AWSV4SignerAuth(credentials, region, service)\n",
    "\n",
    "sts = session.client(\"sts\")\n",
    "identity = sts.get_caller_identity()\n",
    "print(\"Running as IAM identity:\", identity[\"Arn\"])\n",
    "\n",
    "aoss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_compress=True,\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    pool_maxsize=20,\n",
    "    timeout=36000,  # Set request timeout in seconds\n",
    ")\n",
    "\n",
    "# Check if index exists\n",
    "try:\n",
    "    exists = aoss_client.indices.exists(index=index_name)\n",
    "    print(f\"Index {index_name} exists: {exists}\")\n",
    "    if not exists:\n",
    "        raise {\"statusCode\": 500, \"body\": json.dumps(f\"Index {index_name} does not exist\")}\n",
    "except Exception as e:\n",
    "    print(f\"Error checking index: {str(e)}\")\n",
    "    raise {\"statusCode\": 500, \"body\": json.dumps(f\"Error checking index: {str(e)}\")}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opensearch Index:\n",
    "\n",
    "### Info Index Construction fed-court-chunks-index\n",
    "\"\"\"\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"knn\": true\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"embedding\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 512,\n",
    "        \"method\": {\n",
    "          \"name\": \"hnsw\",\n",
    "          \"engine\": \"faiss\",\n",
    "          \"space_type\": \"l2\",\n",
    "          \"parameters\": {\n",
    "            \"ef_construction\": 128,\n",
    "            \"m\": 16\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"doc_id\": {\n",
    "        \"type\": \"keyword\"\n",
    "      },\n",
    "      \"chunk_id\": {\n",
    "        \"type\": \"keyword\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../data/split_chunks/part_550.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_550.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_551.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_551.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_552.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_552.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_553.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_553.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_554.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_554.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_555.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_555.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_556.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_556.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_557.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_557.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_558.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_558.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_559.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_559.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_56.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_56.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_560.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_560.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_561.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_561.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_562.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_562.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_563.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_563.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_564.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_564.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_565.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_565.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_566.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_566.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_567.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_567.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_568.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_568.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_569.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_569.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_57.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_57.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_570.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_570.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_571.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_571.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_572.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_572.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_573.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_573.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_574.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_574.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_58.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_58.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_59.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_59.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_6.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_6.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_60.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_60.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_61.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_61.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_62.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_62.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_63.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_63.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_64.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_64.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_65.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_65.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_66.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_66.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_67.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_67.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_68.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_68.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_69.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_69.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_7.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_7.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_70.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_70.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_71.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_71.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_72.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_72.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_73.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_73.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_74.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_74.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_75.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_75.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_76.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_76.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_77.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_77.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_78.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_78.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_79.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_79.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_8.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_8.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_80.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_80.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_81.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_81.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_82.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_82.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_83.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_83.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_84.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_84.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_85.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_85.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_86.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_86.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_87.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_87.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_88.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_88.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_89.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_89.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_9.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_9.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_90.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_90.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_91.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_91.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_92.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_92.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_93.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_93.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_94.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_94.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_95.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_95.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_96.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_96.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_97.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_97.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_98.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_98.jsonl.gz\n",
      "Processing file: ../data/split_chunks/part_99.jsonl.gz\n",
      "Finished processing file: ../data/split_chunks/part_99.jsonl.gz\n",
      "Successfully inserted: 72828\n",
      "Failures: []\n"
     ]
    }
   ],
   "source": [
    "from_file = \"../data/split_chunks/part_550.jsonl.gz\"\n",
    "def generate_docs():\n",
    "    for root, _, files in os.walk(chunks_dir):\n",
    "        for file in sorted(files):\n",
    "            if file.endswith(\".jsonl.gz\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                if file_path >= from_file:\n",
    "                    print(f\"Processing file: {file_path}\")\n",
    "\n",
    "                    # Read the JSON lines from the gzipped file\n",
    "                    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "                        for line in f:\n",
    "                            try:\n",
    "                                data = json.loads(line)\n",
    "                                yield {\n",
    "                                    \"_index\": index_name,\n",
    "                                    \"_source\": {\n",
    "                                        \"embedding\": data[\"embedding\"],\n",
    "                                        \"doc_id\": data[\"doc_id\"],\n",
    "                                        \"chunk_id\": data[\"chunk_id\"]\n",
    "                                    }\n",
    "                                }\n",
    "\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing line: {str(e)}\")\n",
    "                                raise {\"statusCode\": 500, \"body\": json.dumps(f\"Error processing line: {str(e)}\")}\n",
    "                    print(f\"Finished processing file: {file_path}\")\n",
    "\n",
    "# Bulk ingest in chunks of 1000\n",
    "success, failed = opensearchpy.helpers.bulk(\n",
    "    aoss_client,\n",
    "    generate_docs(),\n",
    "    chunk_size=1000,\n",
    "    raise_on_error=True,\n",
    "    stats_only=False\n",
    ")\n",
    "\n",
    "print(f\"Successfully inserted: {success}\")\n",
    "print(f\"Failures: {failed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in all files: 574828\n"
     ]
    }
   ],
   "source": [
    "lines = 0\n",
    "for root, _, files in os.walk(chunks_dir):\n",
    "    for file in sorted(files):\n",
    "        if file.endswith(\".jsonl.gz\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            # Read the JSON lines from the gzipped file\n",
    "            # counts lines in file\n",
    "                # Read the JSON lines from the gzipped file\n",
    "            with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "                line_count = sum(1 for _ in f)\n",
    "                lines += line_count\n",
    "print(f\"Total lines in all files: {lines}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
