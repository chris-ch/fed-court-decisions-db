{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import opensearchpy\n",
    "\n",
    "chunk_max_words = 500\n",
    "model_name = \"distiluse-base-multilingual-cased-v2\"\n",
    "index_name =  os.environ.get('AWS_OPENSEARCH_INDEX_NAME')\n",
    "service = 'aoss'\n",
    "host = os.getenv('AWS_OPENSEARCH_ENDPOINT')\n",
    "region = os.getenv('AWS_REGION')\n",
    "\n",
    "chunks_dir = \"../data/split_chunks\"\n",
    "\n",
    "access_key = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as IAM identity: arn:aws:iam::211125557955:user/Christophe\n",
      "Index fed-court-chunks-index exists: True\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenSearch client\n",
    "\n",
    "\n",
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "awsauth = AWSV4SignerAuth(credentials, region, service)\n",
    "\n",
    "sts = session.client(\"sts\")\n",
    "identity = sts.get_caller_identity()\n",
    "print(\"Running as IAM identity:\", identity[\"Arn\"])\n",
    "\n",
    "aoss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_compress=True,\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    pool_maxsize=20,\n",
    "    timeout=36000,  # Set request timeout in seconds\n",
    ")\n",
    "\n",
    "# Check if index exists\n",
    "try:\n",
    "    exists = aoss_client.indices.exists(index=index_name)\n",
    "    print(f\"Index {index_name} exists: {exists}\")\n",
    "    if not exists:\n",
    "        raise {\"statusCode\": 500, \"body\": json.dumps(f\"Index {index_name} does not exist\")}\n",
    "except Exception as e:\n",
    "    print(f\"Error checking index: {str(e)}\")\n",
    "    raise {\"statusCode\": 500, \"body\": json.dumps(f\"Error checking index: {str(e)}\")}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opensearch Index:\n",
    "\n",
    "### Info Index Construction fed-court-chunks-index\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"knn\": true\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"embedding\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 512,\n",
    "        \"method\": {\n",
    "          \"name\": \"hnsw\",\n",
    "          \"engine\": \"faiss\",\n",
    "          \"space_type\": \"l2\",\n",
    "          \"parameters\": {\n",
    "            \"ef_construction\": 128,\n",
    "            \"m\": 16\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"doc_id\": {\n",
    "        \"type\": \"keyword\"\n",
    "      },\n",
    "      \"chunk_id\": {\n",
    "        \"type\": \"keyword\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opensearchpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m                     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinished processing file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Bulk ingest in chunks of 1000\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m success, failed = \u001b[43mopensearchpy\u001b[49m.helpers.bulk(\n\u001b[32m     31\u001b[39m     aoss_client,\n\u001b[32m     32\u001b[39m     generate_docs(),\n\u001b[32m     33\u001b[39m     chunk_size=\u001b[32m1000\u001b[39m,\n\u001b[32m     34\u001b[39m     raise_on_error=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     35\u001b[39m     stats_only=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     36\u001b[39m )\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccessfully inserted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuccess\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailures: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'opensearchpy' is not defined"
     ]
    }
   ],
   "source": [
    "from_file = \"../data/split_chunks/part_550.jsonl.gz\"\n",
    "def generate_docs():\n",
    "    for root, _, files in os.walk(chunks_dir):\n",
    "        for file in sorted(files):\n",
    "            if file.endswith(\".jsonl.gz\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                if file_path >= from_file:\n",
    "                    print(f\"Processing file: {file_path}\")\n",
    "\n",
    "                    # Read the JSON lines from the gzipped file\n",
    "                    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "                        for line in f:\n",
    "                            try:\n",
    "                                data = json.loads(line)\n",
    "                                yield {\n",
    "                                    \"_index\": index_name,\n",
    "                                    \"_source\": {\n",
    "                                        \"embedding\": data[\"embedding\"],\n",
    "                                        \"doc_id\": data[\"doc_id\"],\n",
    "                                        \"chunk_id\": data[\"chunk_id\"]\n",
    "                                    }\n",
    "                                }\n",
    "\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing line: {str(e)}\")\n",
    "                                raise {\"statusCode\": 500, \"body\": json.dumps(f\"Error processing line: {str(e)}\")}\n",
    "                    print(f\"Finished processing file: {file_path}\")\n",
    "\n",
    "# Bulk ingest in chunks of 1000\n",
    "success, failed = opensearchpy.helpers.bulk(\n",
    "    aoss_client,\n",
    "    generate_docs(),\n",
    "    chunk_size=1000,\n",
    "    raise_on_error=True,\n",
    "    stats_only=False\n",
    ")\n",
    "\n",
    "print(f\"Successfully inserted: {success}\")\n",
    "print(f\"Failures: {failed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in all files: 574828\n"
     ]
    }
   ],
   "source": [
    "lines = 0\n",
    "for root, _, files in os.walk(chunks_dir):\n",
    "    for file in sorted(files):\n",
    "        if file.endswith(\".jsonl.gz\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            # Read the JSON lines from the gzipped file\n",
    "            # counts lines in file\n",
    "                # Read the JSON lines from the gzipped file\n",
    "            with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "                line_count = sum(1 for _ in f)\n",
    "                lines += line_count\n",
    "print(f\"Total lines in all files: {lines}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
