{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67859f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/onnx_model/tokenizer_config.json',\n",
       " '../data/onnx_model/special_tokens_map.json',\n",
       " '../data/onnx_model/vocab.txt',\n",
       " '../data/onnx_model/added_tokens.json',\n",
       " '../data/onnx_model/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "onnx_path = \"../data/onnx_model\"\n",
    "os.makedirs(onnx_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# Wrap the full model (including pooling + dense layer)\n",
    "class STEncoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, st_model):\n",
    "        super().__init__()\n",
    "        modules = st_model._modules  # OrderedDict\n",
    "        self.transformer = modules['0'].auto_model\n",
    "        self.pooling = modules['1']\n",
    "        # Grab the inner layers directly\n",
    "        self.linear = modules['2'].linear  # nn.Linear\n",
    "        self.activation = modules['2'].activation_function\n",
    "\n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "        sum_mask = input_mask_expanded.sum(dim=1)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = output[0]\n",
    "        pooled = self.mean_pooling(token_embeddings, attention_mask)\n",
    "        projected = self.activation(self.linear(pooled))\n",
    "        return projected\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = \"sentence-transformers/distiluse-base-multilingual-cased-v2\"\n",
    "model = SentenceTransformer(model_id)\n",
    "wrapper = STEncoderWrapper(model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "dummy_inputs = tokenizer(\"Exporting to ONNX is fun!\", return_tensors=\"pt\")\n",
    "\n",
    "# Export to ONNX â€” this model will output [batch_size, 512]\n",
    "torch.onnx.export(\n",
    "    wrapper,\n",
    "    (dummy_inputs[\"input_ids\"], dummy_inputs[\"attention_mask\"]),\n",
    "    f\"{onnx_path}/model.onnx\",\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"sentence_embedding\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "        \"sentence_embedding\": {0: \"batch_size\"},\n",
    "    },\n",
    "    opset_version=14\n",
    ")\n",
    "\n",
    "# Save tokenizer (optional, but useful)\n",
    "tokenizer.save_pretrained(\"../data/onnx_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
